name: World Leaders Health Monitor
on:
  schedule:
    - cron: '*/30 * * * *' 
  workflow_dispatch:

jobs:
  track:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Fetch Global Leaders Health Data
        shell: python
        run: |
          import json, os, re, urllib.request, urllib.parse
          from datetime import datetime

          def get_val(item, tag):
              match = re.search(f'<{tag}>(.*?)</{tag}>', item, re.DOTALL)
              return match.group(1) if match else ""

          # 1. STRATEJİ: Spesifik Kritik Liderler
          specific_leaders = ["Biden", "Putin", "Erdoğan", "Xi Jinping", "Olaf Scholz", "Macron", "King Charles", "Khamenei", "Netanyahu"]
          
          # 2. STRATEJİ: Ünvan Bazlı Küresel Tarama (Herhangi bir ülkenin başkanı için)
          global_queries = [
              'intitle:"President" (hospitalized OR "health condition" OR surgery OR medical)',
              'intitle:"Prime Minister" (hospitalized OR "health condition" OR surgery OR medical)',
              'intitle:"King" (hospitalized OR "health condition" OR surgery OR medical)',
              'intitle:"Cumhurbaşkanı" (hastaneye kaldırıldı OR "sağlık durumu")',
              'intitle:"Başbakan" (hastaneye kaldırıldı OR "sağlık durumu")'
          ]

          def fetch_data():
              collected = []
              # Önce isimleri tara
              for leader in specific_leaders:
                  q = f'"{leader}" (health OR hospital OR surgery OR medical)'
                  collected.extend(process_query(q, "en-US"))
              
              # Sonra ünvanları tara
              for q in global_queries:
                  collected.extend(process_query(q, "en-US"))
                  if "Cumhurbaşkanı" in q or "Başbakan" in q:
                      collected.extend(process_query(q, "tr-TR"))
              
              return collected

          def process_query(query, lang):
              items_list = []
              encoded_q = urllib.parse.quote(query)
              url = f"https://news.google.com/rss/search?q={encoded_q}+after:2025-01-01&hl={lang}"
              
              try:
                  req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
                  with urllib.request.urlopen(req) as resp:
                      content = resp.read().decode('utf-8')
                      matches = re.findall(r'<item>(.*?)</item>', content, re.DOTALL)
                      for it in matches[:5]:
                          title = get_val(it, 'title').replace('<![CDATA[', '').replace(']]>', '')
                          link = get_val(it, 'link')
                          
                          # FİLTRELEME: Magazin ve Zırva Engeli
                          blacklist = ['onedio', 'magazin', 'astroloji', 'burç', 'dizi', 'fiyatı']
                          if any(x in link.lower() or x in title.lower() for x in blacklist): continue
                          
                          items_list.append({
                              "title": title,
                              "link": link,
                              "pubDate": get_val(it, 'pubDate'),
                              "timestamp": str(datetime.now())
                          })
              except: pass
              return items_list

          # Veriyi çek ve tekilleştir
          raw_data = fetch_data()
          unique_data = []
          seen_links = set()
          for d in raw_data:
              if d['link'] not in seen_links:
                  unique_data.append(d)
                  seen_links.add(d['link'])

          # JSON GÜNCELLEME
          filename = 'leaders_health.json'
          old_items = []
          if os.path.exists(filename):
              try:
                  with open(filename, 'r', encoding='utf-8') as f:
                      old_items = json.load(f).get('items', [])
              except: pass

          all_links = {i['link'] for i in old_items}
          new_entries = [i for i in unique_data if i['link'] not in all_links]
          
          # En yeni 300 haberi sakla
          final_list = (new_entries + old_items)[:300]

          with open(filename, 'w', encoding='utf-8') as f:
              json.dump({"status": "ok", "items": final_list, "last_update": str(datetime.now())}, f, indent=2, ensure_ascii=False)

      - name: Save Data
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "LeaderHealthBot"
          git add leaders_health.json
          git commit -m "Global leader health sync: $(date)" || exit 0
          git push origin main
